{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dims, n_actions, gamma=0.99, tau=0.98):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.input = nn.Linear(*input_dims, 256)\n",
    "        self.dense = nn.Linear(256, 256)\n",
    "\n",
    "        self.gru = nn.GRUCell(256, 256)\n",
    "        self.pi = nn.Linear(256, n_actions)\n",
    "        self.v = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, hx):\n",
    "        x = F.relu(self.input(state))\n",
    "        x = F.relu(self.dense(x))\n",
    "        hx = self.gru(x, (hx))\n",
    "\n",
    "        pi = self.pi(hx)\n",
    "        v = self.v(hx)\n",
    "\n",
    "        probs = T.softmax(pi, dim=1)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.numpy()[0], v, log_prob, hx\n",
    "\n",
    "    def calc_R(self, done, rewards, values):\n",
    "        values = T.cat(values).squeeze()\n",
    "        if len(values.size()) == 1:  # batch of states\n",
    "            R = values[-1] * (1-int(done))\n",
    "        elif len(values.size()) == 0:  # single state\n",
    "            R = values*(1-int(done))\n",
    "\n",
    "        batch_return = []\n",
    "        for reward in rewards[::-1]:\n",
    "            R = reward + self.gamma * R\n",
    "            batch_return.append(R)\n",
    "        batch_return.reverse()\n",
    "        batch_return = T.tensor(batch_return, \n",
    "                                dtype=T.float).reshape(values.size())\n",
    "        return batch_return\n",
    "\n",
    "    def calc_loss(self, new_states, hx, done,\n",
    "                  rewards, values, log_probs, r_i_t=None):\n",
    "        if r_i_t is not None:\n",
    "            rewards += r_i_t.detach().numpy()\n",
    "        returns = self.calc_R(done, rewards, values)\n",
    "        next_v = T.zeros(1, 1) if done else self.forward(T.tensor([new_states],\n",
    "                                         dtype=T.float), hx)[1]\n",
    "\n",
    "        values.append(next_v.detach())\n",
    "        values = T.cat(values).squeeze()\n",
    "        log_probs = T.cat(log_probs)\n",
    "        rewards = T.tensor(rewards)\n",
    "\n",
    "        delta_t = rewards + self.gamma*values[1:] - values[:-1]\n",
    "        n_steps = len(delta_t)\n",
    "        gae = np.zeros(n_steps)\n",
    "        for t in range(n_steps):\n",
    "            for k in range(0, n_steps-t):\n",
    "                temp = (self.gamma*self.tau)**k*delta_t[t+k]\n",
    "                gae[t] += temp\n",
    "        gae = T.tensor(gae, dtype=T.float)\n",
    "\n",
    "        actor_loss = -(log_probs*gae).sum()\n",
    "        entropy_loss = (-log_probs*T.exp(log_probs)).sum()\n",
    "        # [T] vs ()\n",
    "        critic_loss = F.mse_loss(values[:-1].squeeze(), returns)\n",
    "\n",
    "        total_loss = actor_loss + critic_loss - 0.01*entropy_loss\n",
    "        return total_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
